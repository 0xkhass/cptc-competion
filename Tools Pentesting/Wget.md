### Wget

Wget is a command-line utility for retrieving content from web servers. **When to use it:**

- To download files from HTTP, HTTPS, and FTP servers.
- To recursively download an entire website for offline viewing or analysis.
- To mirror a website while converting links for local viewing.
- In scripts to automate the downloading of files or web content. **Quick usage (learning):**

bash

```
# Download a single file
wget https://example.com/file.zip

# Download a file and save it with a different name
wget -O newname.zip https://example.com/file.zip

# Recursively download a website (be careful, this can be loud)
wget --recursive --no-parent https://example.com/
```

**Common flags / options:**

- `-O <file>`: Save the downloaded content to a specific file.
- `-r` or `--recursive`: Enable recursive downloading.
- `-l <depth>`: Specify the recursion depth.
- `--no-parent`: Don't ascend to the parent directory when recursing.
- `-U "<User-Agent>"`: Specify a custom User-Agent string. **Authorized commands (AUTHORIZED‑ONLY):** Advanced actionable guidance requires proof of authorization. **Defensive/detection notes:**
- Monitor web server logs for suspicious User-Agent strings (Wget has a default one) or high-volume, rapid-fire requests characteristic of recursive downloading.
- Rate limiting and IP-based blocking can mitigate aggressive scraping attempts.
- Use a WAF to block bots and known malicious user agents. **Recommended lab exercise:** Use `wget` to download the installer for a tool like `nmap` from its official website. Then, try to recursively download a simple, small website like `info.cern.ch`. **Further reading / official docs:**
- GNU Wget Manual: `https://www.gnu.org/software/wget/manual/wget.html`
- `wget` man page: `man wget`